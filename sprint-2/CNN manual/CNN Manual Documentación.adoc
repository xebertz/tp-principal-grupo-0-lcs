= CNN implementado manualmente
Ebertz Ximena <xebertz@campus.ungs.edu.ar>; Franco Leandro <leandro00fr@gmail.com>; López Gonzalo <gonzagonzalopez20@gmail.com>; Venditto Pedro <pedrovenditto41@gmail.com>; Villalba Gastón <gastonleovillalba@gmail.com>;
v1, {docdate}
:toc:
:title-page:
:toc-title: Secciones
:numbered:
:source-highlighter: highlight.js
:tabsize: 4
:nofooter:
:pdf-page-margin: [2.8cm, 2.8cm, 2.8cm, 2.8cm]

== ¿Qué es una Red Neuronal Convolucional?
Las CNN son una clase especializada de redes neuronales profundas diseñadas para procesar y analizar datos de tipo grid, como imágenes y vídeos. Se destacan por su capacidad para capturar patrones espaciales en los datos mediante el uso de capas de convolución.

Cada una de ellas se encuentra conformada por una serie de capas, en donde cada una de ellas tiene una funcionalidad distinta. Primero, están las *capas de convolución*, estas capas aplican filtros sobre las imágenes con el objetivo de poder detectar mas eficientemente los bordes, texturas y formas. Por otro lado, también existen las denominadas *"Capas de Pooling"*, las cuales se utilizan para reducir la dimensionalidad espacial de las representaciones y hacer que la red sea más invariante a pequeñas traslaciones y variaciones. Finalmente, luego de varias capas de convolución y pooling, aparecen las *capas completamente conectadas*, estas capas utilizan las características aprendidas por las capas anteriores para asignar la entrada a clases específicas.

== Modelo Implementado
Nuestro modelo trabaja con un dataset creado por nosotros en base a otros dataset obtenidos de Kaggle. Nuestro dataset puede obtenerse del siguiente link: "https://www.kaggle.com/datasets/gonzajl/tumores-cerebrales-mri-dataset".

La implementación del modelo utilizando redes neuronales convolucionales es similar al que realizó el grupo 1 en el trabajo inicial. Los cambios realizados son principalmente con el objetivo de adaptarlo para que no sea binario, sino multiclase, en particular, el modelo puede clasificar en 4 clases distintas. La primera, es *glioma*, el cual es un tumor que se origina en el tronco encefálico, la médula espinal y el cerebelo. La segunda clasificación posible es *meningioma*, Un meningioma es un tumor que se origina en las membranas que rodean al cerebro y la médula espinal. El tercer tipo posible es el denominado *"pituitary tumors"*, el cual en español significa *"Tumores Hipofisarios"*, estos se encuentran en la base del cerebro. Finalmente, la última clasificación que considera el modelo se denomina *No tumor*, que como dice el nombre, hace referencia a cuando no se detecta ningún tumor en la imágen.

== Adaptación del modelo multiclase

Los cambios que se realizaron para que el modelo pasara de ser binario a multiclase (Considerando las 4 clases mencionadas anteriormente) fueron tres, en donde dos de ellos se encuentran relacionados entre sí. Es decir, uno de los cambios requeridos fueron al final del código, en donde se tuvo que modificar la función de pérdida o "loss". Un modelo de clasificación binaria utiliza un método denominado *binary_crossentropy*, mientras que uno multiclase utiliza otro llamado *categorical_crossentropy*. Ahora, el método categorical_crossentropy requiere que las etiquetas de las imágenes estén en un formato en específico, lo que conlleva una segunda modificación, se agregó una transformación de todas las etiquetas mediante el método *to_categorical*, el cual las adapta para que estas estén en el formato requerido por el método de pérdida. El último cambio se realizó dentro de la sección de activación del modelo, en donde se utiliza una neurona densa denominada *Softmax*, la cual indica que el modelo tiene 4 neuronas en la capa de salida, una para cada una de las clases posibles.

== Entrenamiento y prueba

Se utilizaron distintas distribuciones de las 44 mil imágenes disponibles para entrenar el modelo, por cuestiones de recursos, de esas 44 mil pudimos utilizar eficazmente alrededor de 42 mil imágenes, ya que si se agregaban más tiraba error al momento de la ejecución de algunas secciones del código por falta de recursos.
La distribución más prometedora fue de 33 mil imágenes para entrenamiento y 9 mil imágenes para test, dando un total de 42 mil imágenes en el proceso.

== Mejores resultados

=== 33 mil de entrenamiento y 7 mil de test

Epoch 1/5
loss: 0.7553 - *accuracy: 0.7233* - val_loss: 0.3161 - val_accuracy: 0.9781

Epoch 2/5
loss: 0.3979 - *accuracy: 0.8182* - val_loss: 0.2234 - val_accuracy: 0.9656

Epoch 3/5
loss: 0.3406 - *accuracy: 0.8502* - val_loss: 0.3888 - val_accuracy: 0.8719

Epoch 4/5
loss: 0.2808 - *accuracy: 0.8857* - val_loss: 0.3090 - val_accuracy: 0.9493

Epoch 5/5
loss: 0.2335 - *accuracy: 0.9062* - val_loss: 0.3512 - val_accuracy: 0.8776

*Resultado:* 90,62% de precisión total.

=== 30 mil entrenamiento y 7 mil de prueba

Epoch 1/5
loss: 0.7548 - *accuracy: 0.7543* - val_loss: 0.3532 - val_accuracy: 0.8371

Epoch 2/5
loss: 0.3512 - *accuracy: 0.8478* - val_loss: 0.2726 - val_accuracy: 0.8806

Epoch 3/5
loss: 0.2897 - *accuracy: 0.8764* - val_loss: 0.2244 - val_accuracy: 0.9123

Epoch 4/5
loss: 0.2479 - *accuracy: 0.8967* - val_loss: 0.1911 - val_accuracy: 0.9291

Epoch 5/5
loss: 0.2184 - *accuracy: 0.9125* - val_loss: 0.1691 - val_accuracy: 0.9367

*Resultado:* 91,125% de precisión total.

=== 33 mil entrenamiento y 9 mil test
Epoch 1/5
loss: 0.7558 - *accuracy: 0.7373* - val_loss: 0.4308 - val_accuracy: 0.8263

Epoch 2/5
loss: 0.3685 - *accuracy: 0.8380* - val_loss: 0.3655 - val_accuracy: 0.8619

Epoch 3/5
loss: 0.3115 - *accuracy: 0.8656* - val_loss: 0.4055 - val_accuracy: 0.8457

Epoch 4/5
loss: 0.2749 -*accuracy: 0.8851*- val_loss: 0.4779 - val_accuracy: 0.8386

Epoch 5/5
loss: 0.2219 - *ccuracy: 0.9113* - val_loss: 0.8749 - val_accuracy: 0.6984

*Resultado:* 91,13% de precisión total.


