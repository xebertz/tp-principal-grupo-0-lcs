= TP Principal - Laboratorio de Construcción de Software: Sprint 5
Ebertz Ximena <xebertz@campus.ungs.edu.ar>; Franco Leandro <leandro00fr@gmail.com>; López Gonzalo <gonzagonzalopez20@gmail.com>; Venditto Pedro <pedrovenditto41@gmail.com>; Villalba Gastón <gastonleovillalba@gmail.com>;
v1, {docdate}
:toc:
:title-page:
:toc-title: Secciones
:numbered:
:source-highlighter: highlight.js
:tabsize: 4
:nofooter:
:pdf-page-margin: [3cm, 3cm, 3cm, 3cm]

== Introducción

Debido a la finalización del desarrollo de modelos y teniendo en cuenta la necesidad de completar la integración de manera satisfactoria y los requerimientos de otros equipos, para este sprint nos pusimos tres objetivos:

1. Testear la integración realizada por todos los equipos.
2. Brindar soporte al resto de los equipos con respecto a problemas en la API o en los modelos.
3. Si hay tiempo de sobra, iniciar la presentación final.

== Objetivos del documento

Luego de establecer los objetivos realizamos la división de tareas, para iniciar con el desarrollo y lograr el cumplimiento de todas las metas.

En este documento expondremos el trabajo realizado durante el sprint, la división de tareas realizada y los distintos problemas que se presentaron durante esta semana de trabajo.

== Metodología de trabajo

Durante los días 08-11-2023 y 09-11-2023, se brindó soporte a los demás equipos. Esto fue así debido a la ausencia de tres miembros del equipo, por motivos de estudio.

Luego del viernes 10-11-2023, se iniciaron tareas de testeo de integración, para verificar el correcto funcionamiento de las funcionalidades con respecto a los modelos desarrollados por AverIA.

Las funcionalidades testeadas desde el frontend fueron la realización del diagnóstico y el feedback del médico a partir de un diagnóstico. Esto último fue reforzado por pedido del equipo de UX-UI.

== Testing

Durante el testing se encontraron los siguientes problemas:

1. El feedback del modelo _lyso_ se devolvía invertido. Es decir, si el profesional determinaba que se trataba de un caso de un paciente con neumonía, desde el frontend nos llegaba que el paciente no tenía neumonía, y viceversa. Si se realizaba un feedback desde la sección de comentarios, éste se almacenaba de forma correcta, al igual que los síntomas del paciente.
+
2. El feedback del modelo _wini_ fallaba en todos los casos en los que el médico seleccionaba la enfermedad correspondiente. Sin embargo, cuando el médico ingresaba un comentario, éste se almacenaba de manera correcta. Los síntomas del paciente se almacenaban de manera correcta.

Para ambos solucionar ambos problemas se habló con el equipo de UX, ya que detectamos que el problema podría provenir de su parte. Llegamos a esta conclusión debido a que desde nuestra API y desde la API del equipo de datos la información se almacenaba de manera correcta.

El feedback del modelo _fred_ funcionaba perfectamente en todos los casos, al igual que los diagnósticos de todos los modelos.

Durante las pruebas, notamos que habían ciertas imágenes que no se podían cargar en la web. El equipo de UX nos mencionó que ellos consideraban una imagen como válida cuando tiene un tamaño de mínimo 225x225 pixeles. Entonces, nuestro problema surgió debido a que las imágenes que utilizábamos eran de 224x224 pixeles. El equipo de frontend modificó esta restricción, para que las imágenes sean de mínimo 224x224 pixeles, debido a que los modelos reciben imágenes de, como mínimo, ese tamaño.

== API

Para facilitar la tarea de testing, se añadió el endpoint `retrieve` a la API. Este endpoint retorna, a partir de un ID, la información almacenada correspondiente a el diagnóstico realizado para esa imagen. De esta forma, se puede saber si la información enviada desde UX se almacena correctamente de una forma más sencilla, ya que previamente se debía descargar toda la información almacenada según el modelo.

== Soporte

Nos llegó una advertencia del equipo de UX, debido a que se había perdido información de los diagnósticos del 4 al 35. En principio, pensamos que se había utilizado el endpoint `delete all` luego de la inserción de los datos. Sin embargo, rápidamente nos dimos cuenta que no fue así.

La causa de este problema fue un reinicio automático no notificado de `Google Cloud`. Para evitar que esto vuelva a ocurrir, se utilizó robots para que realicen `ping` a la API cada un minuto. Esto no permite que la API entre en estado de suspensión, por lo que no se puede realizar un reinicio.

También, realizamos una limpieza de los datos restantes, en conjunto con el equipo de datos. De otra forma, no podíamos garantizar la consistencia de los datos.

Sin embargo, esto no solucionó nuestro problema. La causa de los reinicios automáticos no está clara, por lo que decidimos cambiar el enfoque para resolver el problema. Surgieron dos alternativas:

1. Utilizar un volumen de Google Cloud para garantizar la persistencia de los datos.
2. Utilizar una base de datos.
